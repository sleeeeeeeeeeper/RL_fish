"""A2å®éªŒç»„åˆ†æï¼šç¯å¢ƒå½±å“

åˆ†æä¸åŒç¯å¢ƒç»´åº¦ï¼ˆæ´‹æµã€éšœç¢ç‰©ã€è·ç¦»ï¼‰å¯¹æ€§èƒ½çš„å½±å“
æ¯ä¸ªç»´åº¦å•ç‹¬åˆ†æï¼Œä¸æ··åˆæ¯”è¾ƒ
"""

import os
import sys
import numpy as np
import pandas as pd
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from analysis.data_loader import scan_experiment_results, group_experiments_by
from analysis.metrics_calculator import calculate_all_metrics, extract_eval_metrics
from analysis.visualization import (
    plot_learning_curves,
    plot_grouped_bars,
    plot_radar_chart,
    plot_success_vs_collision_scatter,
    configure_plot_style,
    ALGORITHM_COLORS,
)


def analyze_current_dimension(experiments, figures_dir, output_dir):
    """åˆ†ææ´‹æµç±»å‹å½±å“ (NC/UC/VC)"""
    
    if not experiments:
        print("\nâš ï¸ æ´‹æµç»´åº¦ï¼šæ— å®éªŒæ•°æ®")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š A2.1: æ´‹æµç±»å‹å½±å“åˆ†æ ({len(experiments)}ä¸ªå®éªŒ)")
    print(f"{'='*60}")
    
    exps_with_eval = [e for e in experiments if e.eval_data is not None]
    
    # è¯†åˆ«æ´‹æµç±»å‹å˜ä½“ï¼ˆä»å®éªŒåç§°æå–ï¼‰
    current_types = {}
    for exp in experiments:
        if '_nc_' in exp.exp_name.lower():
            current_type = 'NC (No Current)'
        elif '_uc_' in exp.exp_name.lower():
            current_type = 'UC (Uniform)'
        elif '_vc_' in exp.exp_name.lower():
            current_type = 'VC (Vortex)'
        else:
            current_type = 'Unknown'
        
        if current_type not in current_types:
            current_types[current_type] = []
        current_types[current_type].append(exp)
    
    print(f"  æ´‹æµç±»å‹: {list(current_types.keys())}")
    
    # ä¸ºå®éªŒæ·»åŠ ç®€çŸ­æ˜¾ç¤ºåç§°
    for exp in experiments:
        if '_nc_' in exp.exp_name.lower():
            exp.display_name = 'NC (No Current)'
        elif '_uc_' in exp.exp_name.lower():
            exp.display_name = 'UC (Uniform)'
        elif '_vc_' in exp.exp_name.lower():
            exp.display_name = 'VC (Vortex)'
        else:
            exp.display_name = exp.exp_name
    
    # å›¾1: å­¦ä¹ æ›²çº¿å¯¹æ¯”
    print(f"  [1/4] å­¦ä¹ æ›²çº¿...")
    plot_learning_curves(
        experiments,
        group_by='display_name',
        title='Learning Curves: Ocean Current Types',
        ylabel='Average Return',
        save_path=str(figures_dir / 'a2_current_learning_curves.png'),
        figsize=(10, 6)
    )
    
    # å›¾2: Evalæ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
    if exps_with_eval:
        print(f"  [2/4] Evalæ€§èƒ½å¯¹æ¯”...")
        from matplotlib import pyplot as plt
        
        metrics_to_plot = {
            'Success Rate': 'eval_success_rate',
            'Collision Rate': 'eval_collision_rate',
            'Path Length (m)': 'eval_mean_path_length',
            'Energy': 'eval_mean_energy'
        }
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()
        
        for idx, (metric_name, metric_key) in enumerate(metrics_to_plot.items()):
            ax = axes[idx]
            
            type_names = sorted(current_types.keys())
            values = []
            errors = []
            
            for current_type in type_names:
                type_exps = [e for e in current_types[current_type] if e.eval_data is not None]
                if type_exps:
                    vals = [e.metrics.get(metric_key, 0) for e in type_exps]
                    values.append(np.mean(vals))
                    errors.append(np.std(vals) if len(vals) > 1 else 0)
                else:
                    values.append(0)
                    errors.append(0)
            
            x = np.arange(len(type_names))
            ax.bar(x, values, yerr=errors, capsize=5, alpha=0.7, color='steelblue')
            ax.set_xticks(x)
            ax.set_xticklabels(type_names, rotation=15, ha='right')
            ax.set_ylabel(metric_name)
            ax.set_title(metric_name)
            ax.grid(True, alpha=0.3, axis='y')
        
        plt.suptitle('Ocean Current Impact on Performance', fontsize=14)
        plt.tight_layout()
        plt.savefig(str(figures_dir / 'a2_current_eval_performance.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # å›¾3: é›·è¾¾å›¾
        print(f"  [3/4] å¤šç»´åº¦é›·è¾¾å›¾...")
        radar_data = {}
        for current_type, type_exps in current_types.items():
            type_exps_eval = [e for e in type_exps if e.eval_data is not None]
            if type_exps_eval:
                radar_data[current_type] = {
                    'Success': np.mean([e.metrics.get('eval_success_rate', 0) for e in type_exps_eval]),
                    'Safety': 1.0 - np.mean([e.metrics.get('eval_collision_rate', 0) for e in type_exps_eval]),
                    'Path_Eff': 1.0 / (np.mean([e.metrics.get('eval_mean_path_length', 1) for e in type_exps_eval]) + 1e-6),
                    'Smoothness': 1.0 - np.mean([e.metrics.get('eval_mean_smoothness', 0) for e in type_exps_eval]),  # åè½¬ï¼šè¶Šå°è¶Šå¥½å˜ä¸ºè¶Šå¤§è¶Šå¥½
                    'Energy_Eff': 1.0 / (np.mean([e.metrics.get('eval_mean_energy', 1) for e in type_exps_eval]) + 1e-6),
                }
        
        # å½’ä¸€åŒ–
        categories = ['Success', 'Safety', 'Path_Eff', 'Smoothness', 'Energy_Eff']
        for cat in categories:
            max_val = max(radar_data[t][cat] for t in radar_data.keys())
            if max_val > 0:
                for t in radar_data.keys():
                    radar_data[t][cat] = radar_data[t][cat] / max_val
        
        plot_radar_chart(
            radar_data,
            categories=categories,
            title='Multi-Dimensional Comparison: Ocean Current Types',
            save_path=str(figures_dir / 'a2_current_radar.png'),
            figsize=(8, 8)
        )
        
        # å›¾4: æ ·æœ¬æ•ˆç‡å¯¹æ¯”
        print(f"  [4/4] æ ·æœ¬æ•ˆç‡...")
        eff_data = {}
        for current_type, type_exps in current_types.items():
            eff_data[current_type] = {}
            eff_data[current_type]['SAC'] = np.mean([e.metrics['sample_efficiency'] for e in type_exps]) / 1000
        
        plot_grouped_bars(
            eff_data,
            title='Sample Efficiency: Ocean Current Types',
            xlabel='Current Type',
            ylabel='Steps to 80% (K)',
            save_path=str(figures_dir / 'a2_current_sample_efficiency.png'),
            figsize=(8, 6)
        )
    
    print(f"  âœ… æ´‹æµç±»å‹åˆ†æå®Œæˆ")


def analyze_obstacle_dimension(experiments, figures_dir, output_dir):
    """åˆ†æéšœç¢ç‰©å¯†åº¦å½±å“ (SP/MD/DN/MZ)"""
    
    if not experiments:
        print("\nâš ï¸ éšœç¢ç‰©ç»´åº¦ï¼šæ— å®éªŒæ•°æ®")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š A2.2: éšœç¢ç‰©å¯†åº¦å½±å“åˆ†æ ({len(experiments)}ä¸ªå®éªŒ)")
    print(f"{'='*60}")
    
    exps_with_eval = [e for e in experiments if e.eval_data is not None]
    
    # è¯†åˆ«éšœç¢ç‰©å¯†åº¦ç±»å‹
    obs_types = {}
    for exp in experiments:
        if '_sp_' in exp.exp_name.lower():
            obs_type = 'SP (Sparse)'
        elif '_md_' in exp.exp_name.lower():
            obs_type = 'MD (Medium)'
        elif '_dn_' in exp.exp_name.lower():
            obs_type = 'DN (Dense)'
        elif '_mz_' in exp.exp_name.lower():
            obs_type = 'MZ (Maze)'
        else:
            obs_type = 'Unknown'
        
        if obs_type not in obs_types:
            obs_types[obs_type] = []
        obs_types[obs_type].append(exp)
    
    print(f"  éšœç¢ç‰©ç±»å‹: {list(obs_types.keys())}")
    
    # ä¸ºå®éªŒæ·»åŠ ç®€çŸ­æ˜¾ç¤ºåç§°
    for exp in experiments:
        if '_sp_' in exp.exp_name.lower():
            exp.display_name = 'SP (Sparse)'
        elif '_md_' in exp.exp_name.lower():
            exp.display_name = 'MD (Medium)'
        elif '_dn_' in exp.exp_name.lower():
            exp.display_name = 'DN (Dense)'
        elif '_mz_' in exp.exp_name.lower():
            exp.display_name = 'MZ (Maze)'
        else:
            exp.display_name = exp.exp_name
    
    # å›¾1: å­¦ä¹ æ›²çº¿
    print(f"  [1/4] å­¦ä¹ æ›²çº¿...")
    plot_learning_curves(
        experiments,
        group_by='display_name',
        title='Learning Curves: Obstacle Densities',
        ylabel='Average Return',
        save_path=str(figures_dir / 'a2_obstacle_learning_curves.png'),
        figsize=(10, 6)
    )
    
    # å›¾2-4: ç±»ä¼¼æ´‹æµåˆ†æ
    if exps_with_eval:
        print(f"  [2/4] Evalæ€§èƒ½å¯¹æ¯”...")
        from matplotlib import pyplot as plt
        
        metrics_to_plot = {
            'Success Rate': 'eval_success_rate',
            'Collision Rate': 'eval_collision_rate',
            'Path Length (m)': 'eval_mean_path_length',
            'Energy': 'eval_mean_energy'
        }
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()
        
        for idx, (metric_name, metric_key) in enumerate(metrics_to_plot.items()):
            ax = axes[idx]
            
            type_names = sorted(obs_types.keys())
            values = []
            errors = []
            
            for obs_type in type_names:
                type_exps = [e for e in obs_types[obs_type] if e.eval_data is not None]
                if type_exps:
                    vals = [e.metrics.get(metric_key, 0) for e in type_exps]
                    values.append(np.mean(vals))
                    errors.append(np.std(vals) if len(vals) > 1 else 0)
                else:
                    values.append(0)
                    errors.append(0)
            
            x = np.arange(len(type_names))
            ax.bar(x, values, yerr=errors, capsize=5, alpha=0.7, color='coral')
            ax.set_xticks(x)
            ax.set_xticklabels(type_names, rotation=15, ha='right')
            ax.set_ylabel(metric_name)
            ax.set_title(metric_name)
            ax.grid(True, alpha=0.3, axis='y')
        
        plt.suptitle('Obstacle Density Impact on Performance', fontsize=14)
        plt.tight_layout()
        plt.savefig(str(figures_dir / 'a2_obstacle_eval_performance.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # é›·è¾¾å›¾
        print(f"  [3/4] å¤šç»´åº¦é›·è¾¾å›¾...")
        radar_data = {}
        for obs_type, type_exps in obs_types.items():
            type_exps_eval = [e for e in type_exps if e.eval_data is not None]
            if type_exps_eval:
                radar_data[obs_type] = {
                    'Success': np.mean([e.metrics.get('eval_success_rate', 0) for e in type_exps_eval]),
                    'Safety': 1.0 - np.mean([e.metrics.get('eval_collision_rate', 0) for e in type_exps_eval]),
                    'Path_Eff': 1.0 / (np.mean([e.metrics.get('eval_mean_path_length', 1) for e in type_exps_eval]) + 1e-6),
                    'Smoothness': 1.0 - np.mean([e.metrics.get('eval_mean_smoothness', 0) for e in type_exps_eval]),  # åè½¬ï¼šè¶Šå°è¶Šå¥½å˜ä¸ºè¶Šå¤§è¶Šå¥½
                    'Energy_Eff': 1.0 / (np.mean([e.metrics.get('eval_mean_energy', 1) for e in type_exps_eval]) + 1e-6),
                }
        
        categories = ['Success', 'Safety', 'Path_Eff', 'Smoothness', 'Energy_Eff']
        for cat in categories:
            max_val = max(radar_data[t][cat] for t in radar_data.keys())
            if max_val > 0:
                for t in radar_data.keys():
                    radar_data[t][cat] = radar_data[t][cat] / max_val
        
        plot_radar_chart(
            radar_data,
            categories=categories,
            title='Multi-Dimensional Comparison: Obstacle Densities',
            save_path=str(figures_dir / 'a2_obstacle_radar.png'),
            figsize=(8, 8)
        )
        
        # æ ·æœ¬æ•ˆç‡
        print(f"  [4/4] æ ·æœ¬æ•ˆç‡...")
        eff_data = {}
        for obs_type, type_exps in obs_types.items():
            eff_data[obs_type] = {}
            eff_data[obs_type]['SAC'] = np.mean([e.metrics['sample_efficiency'] for e in type_exps]) / 1000
        
        plot_grouped_bars(
            eff_data,
            title='Sample Efficiency: Obstacle Densities',
            xlabel='Obstacle Density',
            ylabel='Steps to 80% (K)',
            save_path=str(figures_dir / 'a2_obstacle_sample_efficiency.png'),
            figsize=(8, 6)
        )
    
    print(f"  âœ… éšœç¢ç‰©å¯†åº¦åˆ†æå®Œæˆ")


def analyze_distance_dimension(experiments, figures_dir, output_dir):
    """åˆ†æç›®æ ‡è·ç¦»å½±å“ (SD/MD/LD)"""
    
    if not experiments:
        print("\nâš ï¸ è·ç¦»ç»´åº¦ï¼šæ— å®éªŒæ•°æ®")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š A2.3: ç›®æ ‡è·ç¦»å½±å“åˆ†æ ({len(experiments)}ä¸ªå®éªŒ)")
    print(f"{'='*60}")
    
    exps_with_eval = [e for e in experiments if e.eval_data is not None]
    
    # è¯†åˆ«è·ç¦»ç±»å‹
    dist_types = {}
    for exp in experiments:
        if '_sd_' in exp.exp_name.lower():
            dist_type = 'SD (Short)'
        elif '_md_' in exp.exp_name.lower():
            dist_type = 'MD (Medium)'
        elif '_ld_' in exp.exp_name.lower():
            dist_type = 'LD (Long)'
        else:
            dist_type = 'Unknown'
        
        if dist_type not in dist_types:
            dist_types[dist_type] = []
        dist_types[dist_type].append(exp)
    
    print(f"  è·ç¦»ç±»å‹: {list(dist_types.keys())}")
    
    # ä¸ºå®éªŒæ·»åŠ ç®€çŸ­æ˜¾ç¤ºåç§°
    for exp in experiments:
        if '_sd_' in exp.exp_name.lower():
            exp.display_name = 'SD (Short)'
        elif '_md_' in exp.exp_name.lower():
            exp.display_name = 'MD (Medium)'
        elif '_ld_' in exp.exp_name.lower():
            exp.display_name = 'LD (Long)'
        else:
            exp.display_name = exp.exp_name
    
    # å›¾1: å­¦ä¹ æ›²çº¿
    print(f"  [1/4] å­¦ä¹ æ›²çº¿...")
    plot_learning_curves(
        experiments,
        group_by='display_name',
        title='Learning Curves: Goal Distances',
        ylabel='Average Return',
        save_path=str(figures_dir / 'a2_distance_learning_curves.png'),
        figsize=(10, 6)
    )
    
    # å›¾2-4: ç±»ä¼¼å‰é¢åˆ†æ
    if exps_with_eval:
        print(f"  [2/4] Evalæ€§èƒ½å¯¹æ¯”...")
        from matplotlib import pyplot as plt
        
        metrics_to_plot = {
            'Success Rate': 'eval_success_rate',
            'Collision Rate': 'eval_collision_rate',
            'Path Length (m)': 'eval_mean_path_length',
            'Energy': 'eval_mean_energy'
        }
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()
        
        for idx, (metric_name, metric_key) in enumerate(metrics_to_plot.items()):
            ax = axes[idx]
            
            type_names = sorted(dist_types.keys())
            values = []
            errors = []
            
            for dist_type in type_names:
                type_exps = [e for e in dist_types[dist_type] if e.eval_data is not None]
                if type_exps:
                    vals = [e.metrics.get(metric_key, 0) for e in type_exps]
                    values.append(np.mean(vals))
                    errors.append(np.std(vals) if len(vals) > 1 else 0)
                else:
                    values.append(0)
                    errors.append(0)
            
            x = np.arange(len(type_names))
            ax.bar(x, values, yerr=errors, capsize=5, alpha=0.7, color='seagreen')
            ax.set_xticks(x)
            ax.set_xticklabels(type_names, rotation=15, ha='right')
            ax.set_ylabel(metric_name)
            ax.set_title(metric_name)
            ax.grid(True, alpha=0.3, axis='y')
        
        plt.suptitle('Goal Distance Impact on Performance', fontsize=14)
        plt.tight_layout()
        plt.savefig(str(figures_dir / 'a2_distance_eval_performance.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # é›·è¾¾å›¾
        print(f"  [3/4] å¤šç»´åº¦é›·è¾¾å›¾...")
        radar_data = {}
        for dist_type, type_exps in dist_types.items():
            type_exps_eval = [e for e in type_exps if e.eval_data is not None]
            if type_exps_eval:
                radar_data[dist_type] = {
                    'Success': np.mean([e.metrics.get('eval_success_rate', 0) for e in type_exps_eval]),
                    'Safety': 1.0 - np.mean([e.metrics.get('eval_collision_rate', 0) for e in type_exps_eval]),
                    'Path_Eff': 1.0 / (np.mean([e.metrics.get('eval_mean_path_length', 1) for e in type_exps_eval]) + 1e-6),
                    'Smoothness': 1.0 - np.mean([e.metrics.get('eval_mean_smoothness', 0) for e in type_exps_eval]),  # åè½¬ï¼šè¶Šå°è¶Šå¥½å˜ä¸ºè¶Šå¤§è¶Šå¥½
                    'Energy_Eff': 1.0 / (np.mean([e.metrics.get('eval_mean_energy', 1) for e in type_exps_eval]) + 1e-6),
                }
        
        categories = ['Success', 'Safety', 'Path_Eff', 'Smoothness', 'Energy_Eff']
        for cat in categories:
            max_val = max(radar_data[t][cat] for t in radar_data.keys())
            if max_val > 0:
                for t in radar_data.keys():
                    radar_data[t][cat] = radar_data[t][cat] / max_val
        
        plot_radar_chart(
            radar_data,
            categories=categories,
            title='Multi-Dimensional Comparison: Goal Distances',
            save_path=str(figures_dir / 'a2_distance_radar.png'),
            figsize=(8, 8)
        )
        
        # æ ·æœ¬æ•ˆç‡
        print(f"  [4/4] æ ·æœ¬æ•ˆç‡...")
        eff_data = {}
        for dist_type, type_exps in dist_types.items():
            eff_data[dist_type] = {}
            eff_data[dist_type]['SAC'] = np.mean([e.metrics['sample_efficiency'] for e in type_exps]) / 1000
        
        plot_grouped_bars(
            eff_data,
            title='Sample Efficiency: Goal Distances',
            xlabel='Goal Distance',
            ylabel='Steps to 80% (K)',
            save_path=str(figures_dir / 'a2_distance_sample_efficiency.png'),
            figsize=(8, 6)
        )
    
    print(f"  âœ… ç›®æ ‡è·ç¦»åˆ†æå®Œæˆ")


def generate_comprehensive_report(experiments, exps_by_dimension, output_dir, figures_dir):
    """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
    
    print(f"\nğŸ“ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
    
    report_path = output_dir / 'a2_report.md'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# A2å®éªŒç»„åˆ†ææŠ¥å‘Šï¼šç¯å¢ƒå½±å“\n\n")
        f.write("**å®éªŒç›®æ ‡**: åˆ†æä¸åŒç¯å¢ƒç»´åº¦ï¼ˆæ´‹æµã€éšœç¢ç‰©ã€è·ç¦»ï¼‰å¯¹æ€§èƒ½çš„ç‹¬ç«‹å½±å“\n\n")
        f.write("**ç®—æ³•**: SACï¼ˆå›ºå®šç®—æ³•ï¼Œæ§åˆ¶å˜é‡ï¼‰\n\n")
        f.write("---\n\n")
        
        f.write("## 1. å®éªŒæ¦‚è§ˆ\n\n")
        f.write(f"- **æ€»å®éªŒæ•°**: {len(experiments)}\n")
        f.write(f"- **ç ”ç©¶ç»´åº¦**: 3ä¸ªï¼ˆæ´‹æµç±»å‹ã€éšœç¢ç‰©å¯†åº¦ã€ç›®æ ‡è·ç¦»ï¼‰\n")
        f.write(f"- **å®éªŒæ–¹æ³•**: å•å˜é‡æ§åˆ¶ï¼Œé€ä¸€åˆ†æå„ç»´åº¦å½±å“\n\n")
        
        for dim, exps in sorted(exps_by_dimension.items()):
            f.write(f"- **{dim}ç»´åº¦**: {len(exps)}ä¸ªå®éªŒ\n")
        
        f.write("\n---\n\n")
        
        # A2.1: æ´‹æµç±»å‹å½±å“
        f.write("## 2. A2.1: æ´‹æµç±»å‹å½±å“åˆ†æ\n\n")
        f.write("**ç ”ç©¶é—®é¢˜**: ä¸åŒæ´‹æµç¯å¢ƒï¼ˆæ— æ´‹æµ/å‡åŒ€æ´‹æµ/æ¶¡æ—‹æ´‹æµï¼‰å¦‚ä½•å½±å“æ€§èƒ½ï¼Ÿ\n\n")
        
        f.write("### 2.1.1 å­¦ä¹ æ›²çº¿\n\n")
        f.write("![Current Learning Curves](figures/a2_current_learning_curves.png)\n\n")
        
        f.write("### 2.1.2 æ€§èƒ½å¯¹æ¯”\n\n")
        f.write("![Current Performance](figures/a2_current_eval_performance.png)\n\n")
        f.write("*å¯¹æ¯”ï¼šæˆåŠŸç‡ã€ç¢°æ’ç‡ã€è·¯å¾„é•¿åº¦ã€èƒ½é‡æ¶ˆè€—*\n\n")
        
        f.write("### 2.1.3 å¤šç»´åº¦å¯¹æ¯”\n\n")
        f.write("![Current Radar](figures/a2_current_radar.png)\n\n")
        
        f.write("### 2.1.4 æ ·æœ¬æ•ˆç‡\n\n")
        f.write("![Current Efficiency](figures/a2_current_sample_efficiency.png)\n\n")
        
        f.write("---\n\n")
        
        # A2.2: éšœç¢ç‰©å¯†åº¦å½±å“
        f.write("## 3. A2.2: éšœç¢ç‰©å¯†åº¦å½±å“åˆ†æ\n\n")
        f.write("**ç ”ç©¶é—®é¢˜**: ä¸åŒéšœç¢ç‰©å¯†åº¦ï¼ˆç¨€ç–/ä¸­ç­‰/å¯†é›†/è¿·å®«ï¼‰å¦‚ä½•å½±å“æ€§èƒ½ï¼Ÿ\n\n")
        
        f.write("### 3.1 å­¦ä¹ æ›²çº¿\n\n")
        f.write("![Obstacle Learning Curves](figures/a2_obstacle_learning_curves.png)\n\n")
        
        f.write("### 3.2 æ€§èƒ½å¯¹æ¯”\n\n")
        f.write("![Obstacle Performance](figures/a2_obstacle_eval_performance.png)\n\n")
        
        f.write("### 3.3 å¤šç»´åº¦å¯¹æ¯”\n\n")
        f.write("![Obstacle Radar](figures/a2_obstacle_radar.png)\n\n")
        
        f.write("### 3.4 æ ·æœ¬æ•ˆç‡\n\n")
        f.write("![Obstacle Efficiency](figures/a2_obstacle_sample_efficiency.png)\n\n")
        
        f.write("---\n\n")
        
        # A2.3: ç›®æ ‡è·ç¦»å½±å“
        f.write("## 4. A2.3: ç›®æ ‡è·ç¦»å½±å“åˆ†æ\n\n")
        f.write("**ç ”ç©¶é—®é¢˜**: ä¸åŒç›®æ ‡è·ç¦»ï¼ˆçŸ­/ä¸­/é•¿ï¼‰å¦‚ä½•å½±å“æ€§èƒ½ï¼Ÿ\n\n")
        
        f.write("### 4.1 å­¦ä¹ æ›²çº¿\n\n")
        f.write("![Distance Learning Curves](figures/a2_distance_learning_curves.png)\n\n")
        
        f.write("### 4.2 æ€§èƒ½å¯¹æ¯”\n\n")
        f.write("![Distance Performance](figures/a2_distance_eval_performance.png)\n\n")
        
        f.write("### 4.3 å¤šç»´åº¦å¯¹æ¯”\n\n")
        f.write("![Distance Radar](figures/a2_distance_radar.png)\n\n")
        
        f.write("### 4.4 æ ·æœ¬æ•ˆç‡\n\n")
        f.write("![Distance Efficiency](figures/a2_distance_sample_efficiency.png)\n\n")
        
        f.write("---\n\n")
        
        # ç»¼åˆå¯¹æ¯”
        exps_with_eval = [e for e in experiments if e.eval_data is not None]
        if exps_with_eval:
            f.write("## 5. ç»¼åˆå¯¹æ¯”\n\n")
            f.write("### 5.1 æˆåŠŸç‡vså®‰å…¨æ€§æƒè¡¡\n\n")
            f.write("![All Dimensions Scatter](figures/a2_all_dimensions_scatter.png)\n\n")
            f.write("*å±•ç¤ºä¸‰ä¸ªç»´åº¦æ‰€æœ‰å®éªŒåœ¨æˆåŠŸç‡ä¸ç¢°æ’ç‡ç©ºé—´çš„åˆ†å¸ƒ*\n\n")
        
        f.write("---\n\n")
        
        f.write("## 6. ç»“è®ºä¸å»ºè®®\n\n")
        f.write("1. **æ´‹æµå½±å“**: å¾…åˆ†æç»“æœå¡«å……\n")
        f.write("2. **éšœç¢ç‰©å½±å“**: å¾…åˆ†æç»“æœå¡«å……\n")
        f.write("3. **è·ç¦»å½±å“**: å¾…åˆ†æç»“æœå¡«å……\n")
        f.write("4. **ç»¼åˆå»ºè®®**: åŸºäºå•å˜é‡åˆ†æç»“æœï¼Œæä¾›ç¯å¢ƒè®¾è®¡å»ºè®®\n\n")
        
        f.write("---\n\n")
        f.write(f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}*\n")
    
    print(f"  åˆ†ææŠ¥å‘Š: {report_path}")
    
    # ç”Ÿæˆæ±‡æ€»è¡¨
    summary_data = []
    for exp in experiments:
        row = {
            'Experiment': exp.exp_name,
            'Dimension': exp.env_dimension,
            'Final_Mean': exp.metrics['final_mean'],
            'Sample_Efficiency': exp.metrics['sample_efficiency'],
        }
        if exp.eval_data:
            row.update({
                'Eval_Success': exp.metrics.get('eval_success_rate', np.nan),
                'Eval_Collision': exp.metrics.get('eval_collision_rate', np.nan),
                'Eval_Path_Length': exp.metrics.get('eval_mean_path_length', np.nan),
            })
        summary_data.append(row)
    
    df = pd.DataFrame(summary_data)
    summary_csv = output_dir / 'a2_detailed_summary.csv'
    df.to_csv(summary_csv, index=False, float_format='%.4f')
    print(f"  æ±‡æ€»è¡¨: {summary_csv}")


def main():
    """ä¸»å‡½æ•°ï¼šA2ç¯å¢ƒå½±å“åˆ†æ"""
    
    print("="*80)
    print("A2 å®éªŒç»„åˆ†æï¼šç¯å¢ƒå½±å“åˆ†æï¼ˆåˆ†ç»´åº¦ç‹¬ç«‹åˆ†æï¼‰")
    print("="*80)
    
    # é…ç½®è·¯å¾„
    results_dir = Path(__file__).parent.parent / 'results'
    a2_dir = results_dir / 'a2'
    output_dir = a2_dir / 'analysis'
    figures_dir = output_dir / 'figures'
    
    output_dir.mkdir(parents=True, exist_ok=True)
    figures_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ“‚ åŠ è½½å®éªŒæ•°æ®...")
    experiments = scan_experiment_results(str(results_dir), experiment_group='a2')
    
    if not experiments:
        print("âŒ æœªæ‰¾åˆ°A2å®éªŒæ•°æ®ï¼")
        return
    
    print(f"\nâœ… æˆåŠŸåŠ è½½ {len(experiments)} ä¸ªå®éªŒ")
    
    # è®¡ç®—æŒ‡æ ‡ï¼ˆåŒ…æ‹¬evalæ•°æ®ï¼‰
    print(f"\nğŸ“Š è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
    for exp in experiments:
        metrics = calculate_all_metrics(exp.timesteps, exp.results, exp.ep_lengths)
        exp.metrics = metrics
        
        # å¦‚æœæœ‰evalæ•°æ®ï¼Œæå–å¹¶æ·»åŠ 
        if exp.eval_data is not None:
            eval_metrics = extract_eval_metrics(exp.eval_data)
            exp.metrics.update(eval_metrics)
            print(f"  {exp.exp_name}: eval_success={eval_metrics.get('eval_success_rate', 0):.2%}")
        else:
            print(f"  {exp.exp_name}: [no eval data]")
    
    exps_with_eval = [e for e in experiments if e.eval_data is not None]
    print(f"\n  âœ… {len(exps_with_eval)}/{len(experiments)} ä¸ªå®éªŒåŒ…å«evalæ•°æ®")
    
    # æŒ‰ç¯å¢ƒç»´åº¦åˆ†ç»„
    exps_by_dimension = group_experiments_by(experiments, by='env_dimension')
    
    print(f"\næŒ‰ç¯å¢ƒç»´åº¦åˆ†ç»„:")
    for dim, exps in sorted(exps_by_dimension.items()):
        print(f"  {dim}: {len(exps)} ä¸ªå®éªŒ")
    
    configure_plot_style()
    
    # ========== åˆ†æå„ç»´åº¦ ==========
    
    # A2.1: æ´‹æµç±»å‹å½±å“
    analyze_current_dimension(
        exps_by_dimension.get('current', []),
        figures_dir,
        output_dir
    )
    
    # A2.2: éšœç¢ç‰©å¯†åº¦å½±å“
    analyze_obstacle_dimension(
        exps_by_dimension.get('obstacle', []),
        figures_dir,
        output_dir
    )
    
    # A2.3: ç›®æ ‡è·ç¦»å½±å“
    analyze_distance_dimension(
        exps_by_dimension.get('distance', []),
        figures_dir,
        output_dir
    )
    
    # ç»¼åˆåˆ†æï¼ˆä»…æ•£ç‚¹å›¾ï¼‰
    if exps_with_eval:
        print(f"\nğŸ¨ ç”Ÿæˆç»¼åˆå¯¹æ¯”å›¾...")
        plot_success_vs_collision_scatter(
            exps_with_eval,
            group_by='env_dimension',
            title='Success Rate vs Collision Rate (All Dimensions)',
            save_path=str(figures_dir / 'a2_all_dimensions_scatter.png'),
            figsize=(10, 8)
        )
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    generate_comprehensive_report(
        experiments,
        exps_by_dimension,
        output_dir,
        figures_dir
    )
    
    print("\n" + "="*80)
    print("âœ… A2å®éªŒç»„åˆ†æå®Œæˆï¼")
    print("="*80)
    print()


if __name__ == '__main__':
    main()
