"""A1å®éªŒç»„åˆ†æï¼šç®—æ³•å¯¹æ¯”

åˆ†æPPOã€SACã€TD3ä¸‰ç§ç®—æ³•åœ¨5ä¸ªéš¾åº¦çº§åˆ«ä¸‹çš„æ€§èƒ½è¡¨ç°
"""

import os
import sys
import numpy as np
import pandas as pd
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from analysis.data_loader import scan_experiment_results, filter_experiments, group_experiments_by
from analysis.metrics_calculator import calculate_all_metrics, compare_algorithms, extract_eval_metrics
from analysis.visualization import (
    plot_learning_curves,
    plot_grouped_bars,
    plot_radar_chart,
    plot_multi_panel_learning_curves,
    plot_eval_metrics_comparison,
    plot_success_vs_collision_scatter,
    configure_plot_style,
    ALGORITHM_COLORS
)


def main():
    """ä¸»å‡½æ•°ï¼šA1ç®—æ³•å¯¹æ¯”åˆ†æ"""
    
    print("="*80)
    print("A1 å®éªŒç»„åˆ†æï¼šç®—æ³•å¯¹æ¯” (PPO vs SAC vs TD3)")
    print("="*80)
    
    # é…ç½®è·¯å¾„
    results_dir = Path(__file__).parent.parent / 'results'
    a1_dir = results_dir / 'a1'
    output_dir = a1_dir / 'analysis'
    figures_dir = output_dir / 'figures'
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)
    figures_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ“‚ åŠ è½½å®éªŒæ•°æ®...")
    print(f"ç»“æœç›®å½•: {a1_dir}")
    
    # 1. åŠ è½½æ‰€æœ‰A1å®éªŒæ•°æ®
    experiments = scan_experiment_results(str(results_dir), experiment_group='a1')
    
    if not experiments:
        print("âŒ æœªæ‰¾åˆ°A1å®éªŒæ•°æ®ï¼")
        return
    
    print(f"\nâœ… æˆåŠŸåŠ è½½ {len(experiments)} ä¸ªå®éªŒ")
    
    # 2. è®¡ç®—æ‰€æœ‰å®éªŒçš„æŒ‡æ ‡
    print(f"\nğŸ“Š è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
    for exp in experiments:
        # è®¡ç®—è®­ç»ƒæœŸé—´çš„æŒ‡æ ‡
        metrics = calculate_all_metrics(exp.timesteps, exp.results, exp.ep_lengths)
        exp.metrics = metrics
        
        # å¦‚æœæœ‰evalæ•°æ®ï¼Œæå–å¹¶æ·»åŠ åˆ°metricsä¸­
        if exp.eval_data is not None:
            eval_metrics = extract_eval_metrics(exp.eval_data)
            exp.metrics.update(eval_metrics)
            print(f"  {exp.exp_name}: final_mean={metrics['final_mean']:.2f}, "
                  f"peak={metrics['peak_value']:.2f} @ {metrics['peak_step']}, "
                  f"eval_success={eval_metrics.get('eval_success_rate', 0):.2%}")
        else:
            print(f"  {exp.exp_name}: final_mean={metrics['final_mean']:.2f}, "
                  f"peak={metrics['peak_value']:.2f} @ {metrics['peak_step']}, "
                  f"[no eval data]")
    
    # ç»Ÿè®¡æœ‰evalæ•°æ®çš„å®éªŒ
    exps_with_eval = [e for e in experiments if e.eval_data is not None]
    print(f"\n  âœ… {len(exps_with_eval)}/{len(experiments)} ä¸ªå®éªŒåŒ…å«evalæ•°æ®")
    
    # 3. æŒ‰ç®—æ³•å’Œéš¾åº¦åˆ†ç»„
    print(f"\nğŸ“‹ æŒ‰ç®—æ³•å’Œéš¾åº¦åˆ†ç»„...")
    
    algorithms = ['PPO', 'SAC', 'TD3']
    difficulties = ['L1', 'L2', 'L3', 'L4', 'L5']
    
    # æŒ‰éš¾åº¦åˆ†ç»„
    exps_by_difficulty = group_experiments_by(experiments, by='env_difficulty')
    
    # æŒ‰ç®—æ³•åˆ†ç»„
    exps_by_algorithm = group_experiments_by(experiments, by='algorithm')
    
    # æ‰“å°åˆ†ç»„ç»Ÿè®¡
    print(f"\næŒ‰éš¾åº¦åˆ†ç»„:")
    for diff, exps in sorted(exps_by_difficulty.items()):
        print(f"  {diff}: {len(exps)} ä¸ªå®éªŒ")
    
    print(f"\næŒ‰ç®—æ³•åˆ†ç»„:")
    for algo, exps in sorted(exps_by_algorithm.items()):
        print(f"  {algo}: {len(exps)} ä¸ªå®éªŒ")
    
    # ========== 4. ç”Ÿæˆå›¾è¡¨ ==========
    
    configure_plot_style()
    
    print(f"\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # å›¾è¡¨1: å¤šé¢æ¿å­¦ä¹ æ›²çº¿ï¼ˆç®—æ³•Ã—éš¾åº¦ï¼‰
    print(f"\n  [1/5] å¤šé¢æ¿å­¦ä¹ æ›²çº¿...")
    plot_multi_panel_learning_curves(
        exps_by_difficulty,
        algorithms=algorithms,
        save_path=str(figures_dir / 'a1_learning_curves_grid.png'),
        figsize=(15, 10)
    )
    
    # å›¾è¡¨2: æ€»ä½“å­¦ä¹ æ›²çº¿å¯¹æ¯”ï¼ˆæŒ‰ç®—æ³•ï¼‰
    print(f"  [2/5] æ€»ä½“å­¦ä¹ æ›²çº¿å¯¹æ¯”...")
    plot_learning_curves(
        experiments,
        group_by='algorithm',
        title='Learning Curves Comparison (All Difficulties)',
        ylabel='Average Return',
        save_path=str(figures_dir / 'a1_learning_curves_by_algorithm.png'),
        figsize=(10, 6)
    )
    
    # å›¾è¡¨3: åŸºäºEvalæ•°æ®çš„æ€§èƒ½å¯¹æ¯”ï¼ˆç®—æ³•Ã—éš¾åº¦ï¼‰
    print(f"  [3/5] Evalæ€§èƒ½å¯¹æ¯”...")
    
    if exps_with_eval:
        # å‡†å¤‡evalæ€§èƒ½æ•°æ®ï¼ˆ4ä¸ªå­å›¾ï¼šæˆåŠŸç‡ã€ç¢°æ’ç‡ã€è·¯å¾„é•¿åº¦ã€èƒ½é‡ï¼‰
        eval_perf_data = {
            'Success Rate': {},
            'Collision Rate': {},
            'Path Length (m)': {},
            'Energy Consumption': {}
        }
        
        for diff in difficulties:
            diff_exps_eval = [e for e in exps_with_eval if e.env_difficulty == diff]
            
            for metric_name, metric_key_prefix in [
                ('Success Rate', 'eval_success_rate'),
                ('Collision Rate', 'eval_collision_rate'),
                ('Path Length (m)', 'eval_mean_path_length'),
                ('Energy Consumption', 'eval_mean_energy')
            ]:
                eval_perf_data[metric_name][diff] = {}
                
                for algo in algorithms:
                    algo_exps = [e for e in diff_exps_eval if e.algorithm == algo]
                    if algo_exps:
                        avg_val = np.mean([e.metrics.get(metric_key_prefix, 0) for e in algo_exps])
                        eval_perf_data[metric_name][diff][algo] = avg_val
                    else:
                        eval_perf_data[metric_name][diff][algo] = 0.0
        
        # ç»˜åˆ¶4ä¸ªå­å›¾
        from matplotlib import pyplot as plt
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()
        
        for idx, (metric_name, data) in enumerate(eval_perf_data.items()):
            ax = axes[idx]
            
            x_labels = sorted(data.keys())
            x = np.arange(len(x_labels))
            width = 0.25
            
            for i, algo in enumerate(algorithms):
                values = [data[diff].get(algo, 0) for diff in x_labels]
                offset = (i - 1) * width
                color = ALGORITHM_COLORS.get(algo, None)
                ax.bar(x + offset, values, width, label=algo, color=color, alpha=0.8)
            
            ax.set_xlabel('Difficulty Level')
            ax.set_ylabel(metric_name)
            ax.set_title(metric_name)
            ax.set_xticks(x)
            ax.set_xticklabels(x_labels)
            ax.legend()
            ax.grid(True, alpha=0.3, axis='y')
        
        plt.suptitle('Evaluation Performance Comparison', fontsize=14)
        plt.tight_layout()
        plt.savefig(str(figures_dir / 'a1_eval_performance_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
        print(f"    Saved: a1_eval_performance_comparison.png")
    else:
        # å¦‚æœæ²¡æœ‰evalæ•°æ®ï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®
        print(f"    âš ï¸ No eval data, using training data...")
        perf_data = {}
        for diff in difficulties:
            perf_data[diff] = {}
            diff_exps = exps_by_difficulty.get(diff, [])
            
            for algo in algorithms:
                algo_exps = [e for e in diff_exps if e.algorithm == algo]
                if algo_exps:
                    avg_perf = np.mean([e.metrics['final_mean'] for e in algo_exps])
                    perf_data[diff][algo] = avg_perf
                else:
                    perf_data[diff][algo] = 0.0
        
        plot_grouped_bars(
            perf_data,
            title='Final Performance by Algorithm and Difficulty',
            xlabel='Difficulty Level',
            ylabel='Average Return',
            save_path=str(figures_dir / 'a1_eval_performance_comparison.png'),
            figsize=(12, 6)
        )
    
    # å›¾è¡¨4: åŸºäºEvalæ•°æ®çš„é›·è¾¾å›¾ï¼ˆå¤šç»´åº¦å¯¹æ¯”ï¼‰
    print(f"  [4/5] Evalå¤šç»´åº¦é›·è¾¾å›¾...")
    
    if exps_with_eval:
        # å‡†å¤‡é›·è¾¾å›¾æ•°æ®ï¼ˆä½¿ç”¨evalæŒ‡æ ‡ï¼‰
        radar_data = {}
        categories = ['Success_Rate', 'Safety', 'Path_Efficiency', 'Smoothness', 'Energy_Eff']
        
        for algo in algorithms:
            algo_exps_eval = [e for e in exps_with_eval if e.algorithm == algo]
            if not algo_exps_eval:
                continue
            
            # è®¡ç®—å„evalæŒ‡æ ‡çš„å¹³å‡å€¼
            success_rates = [e.metrics.get('eval_success_rate', 0) for e in algo_exps_eval]
            collision_rates = [e.metrics.get('eval_collision_rate', 0) for e in algo_exps_eval]
            path_lengths = [e.metrics.get('eval_mean_path_length', 0) for e in algo_exps_eval]
            smoothness = [e.metrics.get('eval_mean_smoothness', 0) for e in algo_exps_eval]
            energy = [e.metrics.get('eval_mean_energy', 0) for e in algo_exps_eval]
            
            radar_data[algo] = {
                'Success_Rate': np.mean(success_rates),
                'Safety': 1.0 - np.mean(collision_rates),  # è½¬æ¢ä¸ºå®‰å…¨æ€§ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
                'Path_Efficiency': 1.0 / (np.mean(path_lengths) + 1e-6),  # è·¯å¾„è¶ŠçŸ­è¶Šå¥½
                'Smoothness': 1 - np.mean(smoothness),  # åè½¬ï¼šè¶Šå°çš„smoothnesså€¼è¶Šå¥½ï¼Œæ‰€ä»¥éœ€è¦åè½¬
                'Energy_Eff': 1.0 / (np.mean(energy) + 1e-6),  # èƒ½é‡è¶Šå°‘è¶Šå¥½
            }
        
        # å½’ä¸€åŒ–åˆ° [0, 1]
        for cat in categories:
            max_val = max(radar_data[algo][cat] for algo in radar_data.keys())
            if max_val > 0:
                for algo in radar_data.keys():
                    radar_data[algo][cat] = radar_data[algo][cat] / max_val
        
        plot_radar_chart(
            radar_data,
            categories=categories,
            title='Multi-Dimensional Evaluation Performance',
            save_path=str(figures_dir / 'a1_eval_radar_chart.png'),
            figsize=(8, 8)
        )
    else:
        print(f"    âš ï¸ No eval data for radar chart")
    
    # å›¾è¡¨5: æ ·æœ¬æ•ˆç‡å¯¹æ¯”ï¼ˆæŒ‰éš¾åº¦ï¼‰
    print(f"  [5/8] æ ·æœ¬æ•ˆç‡å¯¹æ¯”...")
    
    sample_eff_data = {}
    for diff in difficulties:
        sample_eff_data[diff] = {}
        diff_exps = exps_by_difficulty.get(diff, [])
        
        for algo in algorithms:
            algo_exps = [e for e in diff_exps if e.algorithm == algo]
            if algo_exps:
                avg_eff = np.mean([e.metrics['sample_efficiency'] for e in algo_exps])
                sample_eff_data[diff][algo] = avg_eff / 1000  # è½¬æ¢ä¸ºK steps
            else:
                sample_eff_data[diff][algo] = 0.0
    
    plot_grouped_bars(
        sample_eff_data,
        title='Sample Efficiency by Algorithm and Difficulty',
        xlabel='Difficulty Level',
        ylabel='Steps to 80% Success (K)',
        save_path=str(figures_dir / 'a1_sample_efficiency.png'),
        figsize=(12, 6)
    )
    
    # å›¾è¡¨6: æˆåŠŸç‡vsç¢°æ’ç‡æ•£ç‚¹å›¾ï¼ˆå¦‚æœæœ‰evalæ•°æ®ï¼‰
    if exps_with_eval:
        print(f"  [6/6] æˆåŠŸç‡vsç¢°æ’ç‡...")
        plot_success_vs_collision_scatter(
            exps_with_eval,
            group_by='algorithm',
            title='Success Rate vs Collision Rate (Algorithm Comparison)',
            save_path=str(figures_dir / 'a1_success_vs_collision.png'),
            figsize=(10, 8)
        )
    else:
        print(f"  âš ï¸ è·³è¿‡evalæ•°æ®å›¾è¡¨ï¼ˆæ— å¯ç”¨æ•°æ®ï¼‰")
    
    # ========== 5. ç”Ÿæˆæ±‡æ€»è¡¨æ ¼ ==========
    
    print(f"\nğŸ“„ ç”Ÿæˆæ±‡æ€»è¡¨æ ¼...")
    
    summary_data = []
    
    for exp in experiments:
        row = {
            'Experiment': exp.exp_name,
            'Algorithm': exp.algorithm,
            'Difficulty': exp.env_difficulty,
            'Final_Mean': exp.metrics['final_mean'],
            'Final_Std': exp.metrics['final_std'],
            'Peak_Value': exp.metrics['peak_value'],
            'Peak_Step': exp.metrics['peak_step'],
            'Sample_Efficiency': exp.metrics['sample_efficiency'],
            'Convergence_Steps': exp.metrics['convergence_steps'],
            'Training_Stability': exp.metrics['training_stability'],
            'AUC': exp.metrics['auc'],
        }
        
        # æ·»åŠ evalæŒ‡æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if exp.eval_data is not None:
            row.update({
                'Eval_Success_Rate': exp.metrics.get('eval_success_rate', np.nan),
                'Eval_Collision_Rate': exp.metrics.get('eval_collision_rate', np.nan),
                'Eval_Mean_Path_Length': exp.metrics.get('eval_mean_path_length', np.nan),
                'Eval_Mean_Energy': exp.metrics.get('eval_mean_energy', np.nan),
                'Eval_Mean_Smoothness': exp.metrics.get('eval_mean_smoothness', np.nan),
                'Eval_Min_Obstacle_Dist': exp.metrics.get('eval_mean_min_obstacle_dist', np.nan),
            })
        
        summary_data.append(row)
    
    df = pd.DataFrame(summary_data)
    
    # ä¿å­˜è¯¦ç»†è¡¨æ ¼
    summary_csv = output_dir / 'a1_detailed_summary.csv'
    df.to_csv(summary_csv, index=False, float_format='%.4f')
    print(f"  è¯¦ç»†æ±‡æ€»è¡¨: {summary_csv}")
    
    # ç”ŸæˆæŒ‰ç®—æ³•æ±‡æ€»çš„ç»Ÿè®¡è¡¨
    agg_dict = {
        'Final_Mean': ['mean', 'std'],
        'Peak_Value': ['mean', 'std'],
        'Sample_Efficiency': 'mean',
        'Training_Stability': 'mean',
    }
    
    # å¦‚æœæœ‰evalæ•°æ®ï¼Œæ·»åŠ åˆ°èšåˆä¸­
    if 'Eval_Success_Rate' in df.columns:
        agg_dict.update({
            'Eval_Success_Rate': ['mean', 'std'],
            'Eval_Collision_Rate': ['mean', 'std'],
            'Eval_Mean_Path_Length': ['mean', 'std'],
            'Eval_Mean_Energy': ['mean', 'std'],
        })
    
    algo_summary = df.groupby('Algorithm').agg(agg_dict).round(4)
    
    algo_summary_csv = output_dir / 'a1_algorithm_summary.csv'
    algo_summary.to_csv(algo_summary_csv)
    print(f"  ç®—æ³•æ±‡æ€»è¡¨: {algo_summary_csv}")
    
    # ç”ŸæˆæŒ‰éš¾åº¦æ±‡æ€»çš„ç»Ÿè®¡è¡¨
    diff_agg_dict = {
        'Final_Mean': ['mean', 'std'],
        'Peak_Value': ['mean', 'std'],
        'Sample_Efficiency': 'mean',
    }
    
    if 'Eval_Success_Rate' in df.columns:
        diff_agg_dict.update({
            'Eval_Success_Rate': ['mean', 'std'],
            'Eval_Collision_Rate': ['mean', 'std'],
        })
    
    diff_summary = df.groupby('Difficulty').agg(diff_agg_dict).round(4)
    
    diff_summary_csv = output_dir / 'a1_difficulty_summary.csv'
    diff_summary.to_csv(diff_summary_csv)
    print(f"  éš¾åº¦æ±‡æ€»è¡¨: {diff_summary_csv}")
    
    # ========== 6. ç”Ÿæˆåˆ†ææŠ¥å‘Š ==========
    
    print(f"\nğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    
    report_path = output_dir / 'a1_report.md'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# A1å®éªŒç»„åˆ†ææŠ¥å‘Šï¼šç®—æ³•å¯¹æ¯”\n\n")
        f.write("**å®éªŒç›®æ ‡**: åœ¨5ä¸ªéš¾åº¦çº§åˆ«ä¸‹æ¯”è¾ƒPPOã€SACã€TD3ä¸‰ç§ç®—æ³•çš„æ€§èƒ½\n\n")
        f.write("---\n\n")
        
        f.write("## 1. å®éªŒæ¦‚è§ˆ\n\n")
        f.write(f"- **æ€»å®éªŒæ•°**: {len(experiments)}\n")
        f.write(f"- **ç®—æ³•**: PPO, SAC, TD3\n")
        f.write(f"- **éš¾åº¦çº§åˆ«**: L1 (Easy) â†’ L5 (Expert)\n")
        f.write(f"- **è¯„ä¼°æŒ‡æ ‡**: æœ€ç»ˆæ€§èƒ½ã€å³°å€¼æ€§èƒ½ã€æ ·æœ¬æ•ˆç‡ã€æ”¶æ•›é€Ÿåº¦ã€è®­ç»ƒç¨³å®šæ€§\n\n")
        
        f.write("---\n\n")
        
        f.write("## 2. å…³é”®å‘ç°\n\n")
        
        # RQ1: å“ªä¸ªç®—æ³•æ€§èƒ½æœ€ä¼˜ï¼Ÿ
        f.write("### RQ1: åœ¨ç›¸åŒç¯å¢ƒä¸‹ï¼Œå“ªç§ç®—æ³•æ€§èƒ½æœ€ä¼˜ï¼Ÿ\n\n")
        
        # æŒ‰æœ€ç»ˆæ€§èƒ½æ’åº
        algo_perf = {algo: np.mean([e.metrics['final_mean'] for e in exps])
                    for algo, exps in exps_by_algorithm.items()}
        best_algo = max(algo_perf, key=algo_perf.get)
        
        f.write(f"**ç­”æ¡ˆ**: **{best_algo}** åœ¨ç»¼åˆè¡¨ç°ä¸Šæœ€ä¼˜\n\n")
        f.write("å„ç®—æ³•å¹³å‡æœ€ç»ˆæ€§èƒ½:\n\n")
        for algo in sorted(algo_perf.keys()):
            f.write(f"- **{algo}**: {algo_perf[algo]:.2f}\n")
        
        f.write("\n")
        
        # è¯¦ç»†å¯¹æ¯”
        f.write("### ç®—æ³•è¯¦ç»†å¯¹æ¯”\n\n")
        f.write("```\n")
        f.write(algo_summary.to_string())
        f.write("\n```\n\n")
        
        f.write("---\n\n")
        
        f.write("## 3. å¯è§†åŒ–ç»“æœ\n\n")
        
        f.write("### 3.1 å­¦ä¹ æ›²çº¿\n\n")
        f.write("![Learning Curves Grid](figures/a1_learning_curves_grid.png)\n\n")
        f.write("*å›¾1: å„ç®—æ³•åœ¨ä¸åŒéš¾åº¦ä¸‹çš„å­¦ä¹ æ›²çº¿ï¼ˆè®­ç»ƒæœŸé—´evaluations.npzæ•°æ®ï¼‰*\n\n")
        
        f.write("### 3.2 è¯„ä¼°æ€§èƒ½å¯¹æ¯”\n\n")
        f.write("![Eval Performance Comparison](figures/a1_eval_performance_comparison.png)\n\n")
        f.write("*å›¾2: åŸºäºç‹¬ç«‹è¯„ä¼°çš„æ€§èƒ½å¯¹æ¯”ï¼ˆæˆåŠŸç‡ã€ç¢°æ’ç‡ã€è·¯å¾„é•¿åº¦ã€èƒ½é‡æ¶ˆè€—ï¼‰*\n\n")
        
        f.write("### 3.3 å¤šç»´åº¦è¯„ä¼°å¯¹æ¯”\n\n")
        f.write("![Eval Radar Chart](figures/a1_eval_radar_chart.png)\n\n")
        f.write("*å›¾3: å¤šç»´åº¦è¯„ä¼°æ€§èƒ½é›·è¾¾å›¾ï¼ˆæˆåŠŸç‡ã€å®‰å…¨æ€§ã€è·¯å¾„æ•ˆç‡ã€å¹³æ»‘åº¦ã€èƒ½é‡æ•ˆç‡ï¼‰*\n\n")
        
        f.write("### 3.4 æ ·æœ¬æ•ˆç‡\n\n")
        f.write("![Sample Efficiency](figures/a1_sample_efficiency.png)\n\n")
        f.write("*å›¾4: æ ·æœ¬æ•ˆç‡å¯¹æ¯”ï¼ˆè¾¾åˆ°80%æˆåŠŸç‡æ‰€éœ€æ­¥æ•°ï¼‰*\n\n")
        
        # å¦‚æœæœ‰evalæ•°æ®ï¼Œæ·»åŠ æˆåŠŸç‡vsç¢°æ’ç‡å›¾
        if exps_with_eval:
            f.write("### 3.5 æˆåŠŸç‡ä¸å®‰å…¨æ€§æƒè¡¡\n\n")
            f.write("![Success vs Collision](figures/a1_success_vs_collision.png)\n\n")
            f.write("*å›¾5: æˆåŠŸç‡vsç¢°æ’ç‡æ•£ç‚¹å›¾ï¼ˆå±•ç¤ºç®—æ³•åœ¨æ€§èƒ½ä¸å®‰å…¨æ€§ä¹‹é—´çš„æƒè¡¡ï¼‰*\n\n")
        
        f.write("---\n\n")
        f.write("---\n\n")
        
        # å¦‚æœæœ‰evalæ•°æ®ï¼Œæ·»åŠ é¢å¤–çš„åˆ†æéƒ¨åˆ†
        if exps_with_eval:
            f.write("## 4. Evalè¯„ä¼°æ•°æ®åˆ†æ\n\n")
            
            # è®¡ç®—å¹³å‡evalæŒ‡æ ‡
            eval_stats = {}
            for algo in algorithms:
                algo_exps_eval = [e for e in exps_with_eval if e.algorithm == algo]
                if algo_exps_eval:
                    eval_stats[algo] = {
                        'success': np.mean([e.metrics.get('eval_success_rate', 0) for e in algo_exps_eval]),
                        'collision': np.mean([e.metrics.get('eval_collision_rate', 0) for e in algo_exps_eval]),
                        'path_length': np.mean([e.metrics.get('eval_mean_path_length', 0) for e in algo_exps_eval]),
                        'energy': np.mean([e.metrics.get('eval_mean_energy', 0) for e in algo_exps_eval]),
                    }
            
            f.write("åŸºäº100ä¸ªepisodesçš„ç‹¬ç«‹è¯„ä¼°ç»“æœï¼š\n\n")
            f.write("| ç®—æ³• | æˆåŠŸç‡ | ç¢°æ’ç‡ | å¹³å‡è·¯å¾„é•¿åº¦(m) | å¹³å‡èƒ½é‡æ¶ˆè€— |\n")
            f.write("|------|--------|--------|----------------|-------------|\n")
            for algo in sorted(eval_stats.keys()):
                stats = eval_stats[algo]
                f.write(f"| **{algo}** | {stats['success']:.2%} | {stats['collision']:.2%} | "
                       f"{stats['path_length']:.2f} | {stats['energy']:.2f} |\n")
            
            f.write("\n**å…³é”®å‘ç°**:\n\n")
            
            # æ‰¾å‡ºæœ€ä½³ç®—æ³•
            best_success_algo = max(eval_stats.keys(), key=lambda x: eval_stats[x]['success'])
            best_safety_algo = min(eval_stats.keys(), key=lambda x: eval_stats[x]['collision'])
            best_efficiency_algo = min(eval_stats.keys(), key=lambda x: eval_stats[x]['path_length'])
            
            f.write(f"- **æœ€é«˜æˆåŠŸç‡**: {best_success_algo} ({eval_stats[best_success_algo]['success']:.2%})\n")
            f.write(f"- **æœ€ä½ç¢°æ’ç‡**: {best_safety_algo} ({eval_stats[best_safety_algo]['collision']:.2%})\n")
            f.write(f"- **æœ€çŸ­è·¯å¾„**: {best_efficiency_algo} ({eval_stats[best_efficiency_algo]['path_length']:.2f}m)\n")
            f.write("\n")
            
            f.write("---\n\n")
        
        section_num = 5 if exps_with_eval else 4
        f.write(f"## {section_num}. ç»“è®ºä¸å»ºè®®\n\n")
        
        f.write(f"1. **æœ€ä½³ç®—æ³•**: {best_algo} åœ¨æœ¬ä»»åŠ¡ä¸­è¡¨ç°æœ€å¥½\n")
        f.write("2. **éš¾åº¦å½±å“**: éšç€éš¾åº¦å¢åŠ ï¼Œæ‰€æœ‰ç®—æ³•æ€§èƒ½å‡ä¸‹é™\n")
        f.write("3. **æ ·æœ¬æ•ˆç‡**: SACé€šå¸¸æ”¶æ•›æ›´å¿«ï¼ˆOff-Policyä¼˜åŠ¿ï¼‰\n")
        f.write("4. **ç¨³å®šæ€§**: PPOè®­ç»ƒç›¸å¯¹ç¨³å®šï¼Œä½†å³°å€¼æ€§èƒ½å¯èƒ½ä¸å¦‚SAC/TD3\n\n")
        
        f.write("---\n\n")
        f.write(f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}*\n")
    
    print(f"  åˆ†ææŠ¥å‘Š: {report_path}")
    
    # ========== å®Œæˆ ==========
    
    print("\n" + "="*80)
    print("âœ… A1å®éªŒç»„åˆ†æå®Œæˆï¼")
    print("="*80)
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  - å›¾è¡¨: {figures_dir}")
    print(f"  - æ±‡æ€»è¡¨: a1_detailed_summary.csv")
    print(f"  - åˆ†ææŠ¥å‘Š: a1_report.md")
    print()


if __name__ == '__main__':
    main()
